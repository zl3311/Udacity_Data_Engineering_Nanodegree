{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this project, I designed a STAR-shape schema of the database system, containing the information of U.S. I94 immigration data, country code representation, destination state demographic summary and visa type definitions and constructed the cloud ETL pipeline in **AWS**. Specifically, data is extracted and preprocessed using **Apache Spark (PySpark)** on a provided workspace, dumped into **S3 bucket** and queried using **Redshift**. The goal of this project is to construct the immigration database system efficiently, such that the the query of needed immigration and geographical information is retrieved quickly for further downstream services like analytics. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql.functions import isnan, when, count, col, expr, year, month, date_format\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "I worked with four dataset specifically. \n",
    "\n",
    "- I94 Immigration: This dataset is provided by Udacity, which contains > 3 million rows and is \"big\". It contains information about foreigners who have entered the U.S. without their P.I.I.. I'm using this table as the main fact table and connect other dimension tables to it. \n",
    "- US city demographic: This dataset is also proviced by Udacity, and it contains city-level demographic distributions of U.S.. I aggregated it to the level of **state** and connected it with the entering state of I94 records.\n",
    "- Country code: This dataset is extracted from the given I94 label ```.SAS``` file, which contains the corresponding information of the country code used by the I94 immigration dataset.\n",
    "- Visa definition: This dataset is collected from [here](https://www.trade.gov/i-94-arrivals-program), which contains the definitions of different visa types of the I94 immigration dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Country code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f = f.read().replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 289 entries, 0 to 288\n",
      "Data columns (total 2 columns):\n",
      "country_code    289 non-null int64\n",
      "country_name    289 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.6+ KB\n"
     ]
    }
   ],
   "source": [
    "head = f.index(\"i94cntyl\")\n",
    "tail = f.find(\";\", head, len(f))\n",
    "content = [s.replace(\"'\", \"\") for s in f[head:tail].split(\"\\n\")[1:]]\n",
    "l = []\n",
    "for c in content:\n",
    "    code, country = c.split(\"=\")\n",
    "    code = int(code.strip())\n",
    "    country = country.strip()\n",
    "    l.append((code, country))\n",
    "df_country = pd.DataFrame.from_records(l, columns=[\"country_code\", \"country_name\"])\n",
    "df_country.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### State demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_city_demo = pd.read_csv(\"us-cities-demographics.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 5 columns):\n",
      "state_code            49 non-null object\n",
      "male_population       49 non-null int64\n",
      "female_population     49 non-null int64\n",
      "total_population      49 non-null int64\n",
      "number_of_veterans    49 non-null int64\n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "columns = [\"State Code\",\"Male Population\", \"Female Population\", \"Total Population\", \"Number of Veterans\"]\n",
    "df_state_demo = df_city_demo[columns].groupby(\"State Code\").sum().astype(int).reset_index()\n",
    "df_state_demo.columns = [\"_\".join(c.lower().split(\" \")) for c in columns]\n",
    "df_state_demo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "del df_city_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Visa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"visainfo.txt\") as f:\n",
    "    f = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 2 columns):\n",
      "visa_type    20 non-null object\n",
      "visa_def     20 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 400.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "columns = f[0].split(\";\")\n",
    "content = []\n",
    "for l in f[1:]:\n",
    "    content.append(l.split(\";\"))\n",
    "df_visa = pd.DataFrame(content, columns=columns)\n",
    "df_visa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Option 1: Local laod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_final = spark.read.load('./sas_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "field_list = ['cicid', 'i94cit', 'i94res', 'i94port', 'i94mode', 'i94addr', 'gender', 'visatype']\n",
    "df_final = df_final.select(field_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Option 2: Cloud load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('aws/credentials.cfg'))\n",
    "output_data = config['AWS']['S3_BUCKET']\n",
    "\n",
    "# Building a spark session with a connection to AWS S3\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\",\"org.apache.hadoop:hadoop-aws:2.7.2\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",config['AWS']['AWS_ACCESS_KEY_ID']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",config['AWS']['AWS_SECRET_ACCESS_KEY']) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "month_list = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "field_list = ['cicid', 'i94cit', 'i94res', 'i94port', 'i94mode', 'i94addr', 'gender', 'visatype']\n",
    "\n",
    "for month in month_list:\n",
    "    fname = f'../../data/18-83510-I94-Data-2016/i94_{month}16_sub.sas7bdat'\n",
    "    df_temp = spark.read.format('com.github.saurfang.sas.spark').load(fname)\n",
    "    df_temp = df_temp.select(field_list)\n",
    "   \n",
    "    if month == 'jan':\n",
    "        df_final = df_temp\n",
    "    else:\n",
    "        df_final = df_final.union(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LEFT ANTI JOIN DUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_final = df_final.cache()\n",
    "df_final_dup = df_final.groupby('cicid').count().filter(col('count')>1)\n",
    "df_final2 = df_final.join(df_final_dup, on=['cicid'], how='left_anti')\n",
    "df_final2 = df_final2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# FILL NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_final2 = df_final2.fillna({'i94cit':-1, 'i94mode':-1, 'i94addr':'99'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CONVERT TO INTEGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cast_lst = ['cicid','i94cit','i94res','i94mode']\n",
    "\n",
    "for col_name in cast_lst:\n",
    "    df_final2 = df_final2.withColumn(col_name, col(col_name).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save to .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_final2 = df_final2.coalesce(4)\n",
    "df_final2.write.parquet(f\"{output_data}I94-Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n",
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_final2.count())\n",
    "df_final2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"state_code\", StringType(), True), \\\n",
    "    StructField(\"male_population\", IntegerType(), True), \\\n",
    "    StructField(\"female_population\", IntegerType(), True), \\\n",
    "    StructField(\"total_population\", IntegerType(), True), \\\n",
    "    StructField(\"number_of_veterans\", IntegerType(), True) \\\n",
    "  ])\n",
    "\n",
    "spark_state_demo = sqlCtx.createDataFrame(df_state_demo, schema=schema)\n",
    "spark_state_demo.write.parquet(f\"{output_data}demographics\")\n",
    "\n",
    "print(spark_state_demo.count())\n",
    "spark_state_demo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "root\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"country_code\", IntegerType(), True), \\\n",
    "    StructField(\"country_name\", StringType(), True) \\\n",
    "  ])\n",
    "\n",
    "spark_country = sqlCtx.createDataFrame(df_country, schema=schema)\n",
    "spark_country.write.parquet(f\"{output_data}country\")\n",
    "\n",
    "print(spark_country.count())\n",
    "spark_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "root\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- visa_def: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"visa_type\", StringType(), True), \\\n",
    "    StructField(\"visa_def\", StringType(), True) \\\n",
    "  ])\n",
    "\n",
    "spark_visa = sqlCtx.createDataFrame(df_visa, schema=schema)\n",
    "spark_visa.write.parquet(f\"{output_data}visa\")\n",
    "\n",
    "print(spark_visa.count())\n",
    "spark_visa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_metadata = pd.DataFrame([(\"fact_immigration\", df_final2.count()), \n",
    "                            (\"dim_country\", spark_country.count()), \n",
    "                            (\"dim_visa\", spark_visa.count()), \n",
    "                            (\"dim_state_demo\", spark_state_demo.count())], columns=[\"table\", \"rows\"])\n",
    "df_metadata.to_json(\"./metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Data model is designed as follows:\n",
    "- Fact table:\n",
    "    - fact_immigration: comes from the I94 immigration data.\n",
    "- Dimension table:\n",
    "    - dim_country: comes from the I94 label SAS file.\n",
    "    - dim_visa: comes from the collected visa definitions.\n",
    "    - dim_state_demo: comes from the aggregation of city-level demographic information.\n",
    "    \n",
    "![schema](./DB_schema.svg)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The ETL pipeline is as follows: \n",
    "- Create staging, dimension and fact tables.\n",
    "- Load processed ```.parquet``` files from S3.\n",
    "- Insert the loaded data to the corresponding dimension and fact tables in Redshift.\n",
    "\n",
    "![pipeline](./Pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python ./operations/create_redshift_cluster.py create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python ./operations/create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python ./operations/etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Two data quality checks are performed:\n",
    "- whether the created table is in the workspace.\n",
    "- whether the number of records in each table is identical to the preprocessed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform data quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python ./operations/data_quality.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Delete (the expensive) Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python ./operations/create_redshift_cluster.py delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "Fact table:\n",
    "- fact_immigration:\n",
    "    - **cicid**: unique id for each visitor.\n",
    "    - i94cit: country code for visitor's citizenship.\n",
    "    - i94res: country code for visitor's residence.\n",
    "    - i94port: id of entry port.\n",
    "    - i94mode: mode of entry.\n",
    "    - i94addr: abbreviation of entry state.\n",
    "    - gender: gender of visitor.\n",
    "    - visatype: visa type of entry.\n",
    "\n",
    "Dimension tables:\n",
    "- dim_country:\n",
    "    - **country_code**: code of country.\n",
    "    - country_name: name of country.\n",
    "- dim_visa:\n",
    "    - **visa_type**: type of visa.\n",
    "    - visa_def: definition of visa type.\n",
    "- dim_state_demo:\n",
    "    - **state_code**: abbreviation of state.\n",
    "    - male_population: total population of males in the state.\n",
    "    - female_population: total population of females in the state.\n",
    "    - total_population: male_population: total population of the state.\n",
    "    - number_of_veterans: total number of veterans in the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * The fact table is the majority component in the database, so it is more efficient to store a shortened version of variables in it and link the detailed descriptions in seperate dimension tables. This is an ideal use case of the STAR-schema, where the storage of duplicated information is avoided. Tables are connected via indices (bold in the tables and figures above), and they can be joined together to generate more complex insights. For example, if a data scientist would like to know which country has the most incoming F1 student, (s)he can query \n",
    "    ```\n",
    "    SELECT \n",
    "          c.country_name AS Country, \n",
    "          SUM(i.visatype=1) AS Students \n",
    "    FROM fact_immigration i \n",
    "    JOIN dim_country c \n",
    "    ON i.i94cit = c.country_code \n",
    "    GROUP BY Country \n",
    "    ORDER BY Students DESC LIMIT 1\n",
    "    ```\n",
    "    * Since the size of I94 data is large, I'm using Apache Spark for processing. The other datasets are much smaller, I'm simply using pandas for processing.\n",
    "    * I'm saving processed data as ```.parquet``` format in S3 bucket storage, because the columnar storage format of ```.parquet``` files would significantly boost the access speed of queries and S3 works natively well with Redshift. \n",
    "    * Redshift is a power tool for data warehousing. It is scalable, distributed and cost-effective, and it is ideal to host the DB on it.\n",
    "* Propose how often the data should be updated and why.\n",
    "    * Normally speaking, the a weekly or a monthly update of data should be fine if the query is not emergent.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x: Process all dataset on cloud. Since the other procedures are already on cloud and in distributed manners, the rest procedures should be scalable enough. \n",
    "    * The data populates a dashboard that must be updated on a daily basis by 7am every day: Schedule the execution of pipelines using tools like Apache Airflow.\n",
    "    * The database needed to be accessed by 100+ people: Change the number and type of hosting nodes in Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
